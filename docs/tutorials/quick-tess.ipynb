{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa40cdac",
   "metadata": {},
   "source": [
    "(quick-tess)=\n",
    "\n",
    "# Quick fits for TESS light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exoplanet\n",
    "\n",
    "exoplanet.utils.docs_setup()\n",
    "print(f\"exoplanet.__version__ = '{exoplanet.__version__}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf1ae2",
   "metadata": {},
   "source": [
    "In this tutorial, we will fit the TESS light curve for a known transiting planet.\n",
    "While the {ref}`tess` case study goes through the full details of an end-to-end fit, this tutorial is significantly faster to run and it can give pretty excellent results depending on your goals.\n",
    "Some of the main differences are:\n",
    "\n",
    "1. We start from the light curve rather than doing the photometry ourselves. This should pretty much always be fine unless you have a very bright, faint, or crowded target.\n",
    "2. We assume a circluar orbit, but as you'll see later, we can approximately relax this assumption later.\n",
    "3. We only fit the data near transit. In many cases this will be just fine, but if you have predictable stellar variability (like coherent rotation) then you might do better fitting more data.\n",
    "\n",
    "We'll fit the planet in the HD 118203 (TIC 286923464) system that was found to transit by [Pepper et al. (2019)](https://arxiv.org/abs/1911.05150) because it is on an eccentric orbit so assumption #2 above is not valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082de1b4",
   "metadata": {},
   "source": [
    "First, let's download the TESS light curve using [lightkurve](https://docs.lightkurve.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightkurve as lk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lcfs = lk.search_lightcurve(\n",
    "    \"TIC 286923464\", mission=\"TESS\", author=\"SPOC\"\n",
    ").download_all(flux_column=\"pdcsap_flux\")\n",
    "lc = lcfs.stitch().remove_nans().remove_outliers(sigma=7)\n",
    "\n",
    "x = np.ascontiguousarray(lc.time.value, dtype=np.float64)\n",
    "y = np.ascontiguousarray(1e3 * (lc.flux - 1), dtype=np.float64)\n",
    "yerr = np.ascontiguousarray(1e3 * lc.flux_err, dtype=np.float64)\n",
    "\n",
    "texp = np.min(np.diff(x))\n",
    "\n",
    "plt.plot(x, y, \"k\", linewidth=0.5)\n",
    "plt.xlabel(\"time [days]\")\n",
    "_ = plt.ylabel(\"relative flux [ppt]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9429fd1",
   "metadata": {},
   "source": [
    "Then, find the period, phase and depth of the transit using box least squares:stitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exoplanet as xo\n",
    "\n",
    "pg = xo.estimators.bls_estimator(x, y, yerr, min_period=2, max_period=20)\n",
    "\n",
    "peak = pg[\"peak_info\"]\n",
    "period_guess = peak[\"period\"]\n",
    "t0_guess = peak[\"transit_time\"]\n",
    "depth_guess = peak[\"depth\"]\n",
    "\n",
    "plt.plot(pg[\"bls\"].period, pg[\"bls\"].power, \"k\", linewidth=0.5)\n",
    "plt.axvline(period_guess, alpha=0.3, linewidth=5)\n",
    "plt.xlabel(\"period [days]\")\n",
    "plt.ylabel(\"bls power\")\n",
    "plt.yticks([])\n",
    "_ = plt.xlim(pg[\"bls\"].period.min(), pg[\"bls\"].period.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9015c13",
   "metadata": {},
   "source": [
    "Then, for efficiency purposes, let's extract just the data within 0.25 days of the transits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transit_mask = (\n",
    "    np.abs(\n",
    "        (x - t0_guess + 0.5 * period_guess) % period_guess - 0.5 * period_guess\n",
    "    )\n",
    "    < 0.25\n",
    ")\n",
    "x = np.ascontiguousarray(x[transit_mask])\n",
    "y = np.ascontiguousarray(y[transit_mask])\n",
    "yerr = np.ascontiguousarray(yerr[transit_mask])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "x_fold = (\n",
    "    x - t0_guess + 0.5 * period_guess\n",
    ") % period_guess - 0.5 * period_guess\n",
    "plt.scatter(x_fold, y, c=x, s=3)\n",
    "plt.xlabel(\"time since transit [days]\")\n",
    "plt.ylabel(\"relative flux [ppt]\")\n",
    "plt.colorbar(label=\"time [days]\")\n",
    "_ = plt.xlim(-0.25, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66d3a0",
   "metadata": {},
   "source": [
    "That looks a little janky, but it's good enough for now.\n",
    "\n",
    "## The probabilistic model\n",
    "\n",
    "Here's how we set up the PyMC3 model in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import aesara_theano_fallback.tensor as tt\n",
    "\n",
    "import pymc3_ext as pmx\n",
    "from celerite2.theano import terms, GaussianProcess\n",
    "\n",
    "\n",
    "with pm.Model() as model:\n",
    "\n",
    "    # Stellar parameters\n",
    "    mean = pm.Normal(\"mean\", mu=0.0, sigma=10.0)\n",
    "    u = xo.QuadLimbDark(\"u\")\n",
    "    star_params = [mean, u]\n",
    "\n",
    "    # Gaussian process noise model\n",
    "    sigma = pm.InverseGamma(\"sigma\", alpha=3.0, beta=2 * np.median(yerr))\n",
    "    log_sigma_gp = pm.Normal(\"log_sigma_gp\", mu=0.0, sigma=10.0)\n",
    "    log_rho_gp = pm.Normal(\"log_rho_gp\", mu=np.log(10.0), sigma=10.0)\n",
    "    kernel = terms.SHOTerm(\n",
    "        sigma=tt.exp(log_sigma_gp), rho=tt.exp(log_rho_gp), Q=1.0 / 3\n",
    "    )\n",
    "    noise_params = [sigma, log_sigma_gp, log_rho_gp]\n",
    "\n",
    "    # Planet parameters\n",
    "    log_ror = pm.Normal(\n",
    "        \"log_ror\", mu=0.5 * np.log(depth_guess * 1e-3), sigma=10.0\n",
    "    )\n",
    "    ror = pm.Deterministic(\"ror\", tt.exp(log_ror))\n",
    "\n",
    "    # Orbital parameters\n",
    "    log_period = pm.Normal(\"log_period\", mu=np.log(period_guess), sigma=1.0)\n",
    "    period = pm.Deterministic(\"period\", tt.exp(log_period))\n",
    "    t0 = pm.Normal(\"t0\", mu=t0_guess, sigma=1.0)\n",
    "    log_dur = pm.Normal(\"log_dur\", mu=np.log(0.1), sigma=10.0)\n",
    "    dur = pm.Deterministic(\"dur\", tt.exp(log_dur))\n",
    "    b = xo.distributions.ImpactParameter(\"b\", ror=ror)\n",
    "\n",
    "    # Set up the orbit\n",
    "    orbit = xo.orbits.KeplerianOrbit(period=period, duration=dur, t0=t0, b=b)\n",
    "\n",
    "    # We're going to track the implied density for reasons that will become clear later\n",
    "    pm.Deterministic(\"rho_circ\", orbit.rho_star)\n",
    "\n",
    "    # Set up the mean transit model\n",
    "    star = xo.LimbDarkLightCurve(u)\n",
    "    lc_model = mean + 1e3 * tt.sum(\n",
    "        star.get_light_curve(orbit=orbit, r=ror, t=x), axis=-1\n",
    "    )\n",
    "\n",
    "    # Finally the GP observation model\n",
    "    gp = GaussianProcess(kernel, t=x, diag=yerr ** 2 + sigma ** 2)\n",
    "    gp.marginal(\"obs\", observed=y - lc_model)\n",
    "\n",
    "    # Double check that everything looks good - we shouldn't see any NaNs!\n",
    "    print(model.check_test_point())\n",
    "\n",
    "    # Optimize the model\n",
    "    map_soln = model.test_point\n",
    "    map_soln = pmx.optimize(map_soln, [sigma])\n",
    "    map_soln = pmx.optimize(map_soln, [ror, b, dur])\n",
    "    map_soln = pmx.optimize(map_soln, noise_params)\n",
    "    map_soln = pmx.optimize(map_soln, star_params)\n",
    "    map_soln = pmx.optimize(map_soln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411499c4",
   "metadata": {},
   "source": [
    "Now we can plot our initial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5463aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    lc_pred = pmx.eval_in_model(lc_model, map_soln)\n",
    "    gp_pred = pmx.eval_in_model(gp.predict(y - lc_pred), map_soln)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "x_fold = (x - map_soln[\"t0\"] + 0.5 * map_soln[\"period\"]) % map_soln[\n",
    "    \"period\"\n",
    "] - 0.5 * map_soln[\"period\"]\n",
    "inds = np.argsort(x_fold)\n",
    "plt.scatter(x_fold, y - gp_pred - map_soln[\"mean\"], c=x, s=3)\n",
    "plt.plot(x_fold[inds], lc_pred[inds] - map_soln[\"mean\"], \"k\")\n",
    "plt.xlabel(\"time since transit [days]\")\n",
    "plt.ylabel(\"relative flux [ppt]\")\n",
    "plt.colorbar(label=\"time [days]\")\n",
    "_ = plt.xlim(-0.25, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c4c01",
   "metadata": {},
   "source": [
    "That looks better!\n",
    "\n",
    "Now on to sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(286923464)\n",
    "with model:\n",
    "    trace = pmx.sample(\n",
    "        tune=1000,\n",
    "        draws=1000,\n",
    "        start=map_soln,\n",
    "        chains=2,\n",
    "        cores=2,\n",
    "        return_inferencedata=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdd932",
   "metadata": {},
   "source": [
    "Then we can take a look at the summary statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7581393",
   "metadata": {},
   "source": [
    "And plot the posterior covariances compared to the values from [Pepper et al. (2019)](https://arxiv.org/abs/1911.05150):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "_ = corner.corner(\n",
    "    trace, var_names=[\"period\", \"ror\", \"b\"], truths=[6.134980, 0.05538, 0.125]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0dac9",
   "metadata": {},
   "source": [
    "## Bonus: eccentricity\n",
    "\n",
    "As discussed above, we fit this model assuming a circular orbit which speeds things up for a few reasons.\n",
    "First, setting eccentricity to zero means that the orbital dynamics are much simpler and more computationally efficient, since we don't need to solve Kepler's equation numerically.\n",
    "But this isn't actually the main effect!\n",
    "Instead the bigger issues come from the fact that the degeneracies between eccentricity, arrgument of periasteron, impact parameter, and planet radius are hard for the sampler to handle, causing the sampler's performance to plummet.\n",
    "In this case, by fitting with a circular orbit where duration is one of the parameters, everything is well behaved and the sampler runs faster.\n",
    "\n",
    "But, in this case, the planet *is* actually on an eccentric orbit, so that assumption isn't justified.\n",
    "It has been recognized by various researchers over the years (I first learned about this from [Bekki Dawson](https://arxiv.org/abs/1203.5537)) that, to first order, the eccentricity mainly just changes the transit duration.\n",
    "The key realization is that this can be thought of as a change in the implied density of the star.\n",
    "Therefore, if you fit the transit using stellar density (or duration, in this case) as one of the parameters (*note: you must have a* different *stellar density parameter for each planet if there are more than one*), you can use an independent measurement of the stellar density to infer the eccentricity of the orbit after the fact.\n",
    "All the details are described in [Dawson & Johnson (2012)](https://arxiv.org/abs/1203.5537), but here's how you can do this here using the stellar density listed in the TESS input catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d941e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.mast import Catalogs\n",
    "\n",
    "star = Catalogs.query_object(\"TIC 286923464\", catalog=\"TIC\", radius=0.001)\n",
    "tic_rho_star = float(star[\"rho\"]), float(star[\"e_rho\"])\n",
    "print(\"rho_star = {0} Â± {1}\".format(*tic_rho_star))\n",
    "\n",
    "# Extract the implied density from the fit\n",
    "rho_circ = np.repeat(np.asarray(trace.posterior[\"rho_circ\"]).flatten(), 100)\n",
    "\n",
    "# Sample eccentricity and omega from their priors (the math might\n",
    "# be a little more subtle for more informative priors, but I leave\n",
    "# that as an exercise for the reader...)\n",
    "ecc = np.random.uniform(0, 1, len(rho_circ))\n",
    "omega = np.random.uniform(-np.pi, np.pi, len(rho_circ))\n",
    "\n",
    "# Compute the \"g\" parameter from Dawson & Johnson and what true\n",
    "# density that implies\n",
    "g = (1 + ecc * np.sin(omega)) / np.sqrt(1 - ecc ** 2)\n",
    "rho = rho_circ / g ** 3\n",
    "\n",
    "# Re-weight these samples to get weighted posterior samples\n",
    "log_weights = -0.5 * ((rho - tic_rho_star[0]) / tic_rho_star[1]) ** 2\n",
    "weights = np.exp(log_weights - np.max(log_weights))\n",
    "\n",
    "# Estimate the expected posterior quantiles\n",
    "q = corner.quantile(ecc, [0.16, 0.5, 0.84], weights=weights)\n",
    "print(\n",
    "    \"eccentricity = {0:.2f} +{1[1]:.2f} -{1[0]:.2f}\".format(q[1], np.diff(q))\n",
    ")\n",
    "\n",
    "_ = corner.corner(\n",
    "    np.vstack((ecc, omega)).T,\n",
    "    weights=weights,\n",
    "    truths=[0.316, None],\n",
    "    plot_datapoints=False,\n",
    "    labels=[\"eccentricity\", \"omega\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60281e25",
   "metadata": {},
   "source": [
    "As you can see, this eccentricity estimate is consistent (albeit with large uncertainties) with the value that [Pepper et al. (2019)](https://arxiv.org/abs/1911.05150) measure using radial velocities and it is definitely clear that this planet is not on a circular orbit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f447c96",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "As described in the [citation tutorial](https://docs.exoplanet.codes/en/stable/tutorials/citation/), we can use [citations.get_citations_for_model](https://docs.exoplanet.codes/en/stable/user/api/#exoplanet.citations.get_citations_for_model) to construct an acknowledgement and BibTeX listing that includes the relevant citations for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80987110",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    txt, bib = xo.citations.get_citations_for_model()\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bib.split(\"\\n\\n\")[0] + \"\\n\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60105085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
